#! /usr/bin/env ruby

require "net/http"
require "uri"
require "pry"
require "json"
require "yaml"

# WARNING: This uses some very unofficial hackery to obtain the data
#
# It abuses the fact that Instagram is using React to build their pages and I
# basically scrape out their data object, and save it to a YAML.
#
# So it's all off the record, on the QT and very hush-hush
#
# Also: Instagram is a bag of dicks. Their API will only allow you to fetch the 20 latest
# images, unless you submit for a review process to get your app in a production state.

pictures_to_fetch = ["https://www.instagram.com/p/BPAEcQIDiJP/?taken-by=junkiesxl"]

if file = ENV["ALL_INSTAGRAMS"]
  # How would one obtain a file with all the URLs of every Instagram picture you ever took?
  #
  # 1. Go to the instagram page where you can view your photos (instagram.com/junkiesxl)
  # 2. Start scrolling and clicking "Load more"
  # 3. Keep on scrolling until the end.
  # 4. Open a console and do:
  #      elements = document.querySelectorAll("._8mlbc._vbtk2._t5r8b")
  #      var urls = Array.prototype.map.call(elements, function(el) { return el.href; })
  #      JSON.stringify(urls)
  # 5. Right click in the console row of `JSON.stringify` and choose "Save"
  # 6. Open that file in your preferred editor and massage it into a instagram URL on each line
  pictures_to_fetch = File.read(File.expand_path(file)).lines.map(&:strip)
end

module Instascraper
  class Graphdata
    attr_accessor :raw, :url

    def self.with_url(url, json)
      thing = new(json["entry_data"]["PostPage"].first["graphql"]["shortcode_media"])
      thing.url = url
      thing
    end

    def self.all_from_yaml(yaml)
      r = yaml.map do |data|
        new(data[:raw])
      end
      def r.franzies
        select { |f| f.caption =~ /le franz/i}
      end
      def r.bobs
        select { |f| f.caption =~ /dear diary/i}
      end
      r
    end

    def initialize(raw)
      @raw = raw
    end

    def instagram_id
      @raw["id"]
    end

    def shortcode
      @raw["shortcode"]
    end

    def caption
      @raw["edge_media_to_caption"]["edges"].first["node"]["text"]
    end

    def likes
      @raw["edge_media_preview_like"]["count"]
    end

    def picture_url
      @raw["display_url"]
    end

    def inspect
      "<#{shortcode}> #{caption} (#{likes})"
    end

    def to_h
      r = {
        instagram_id: instagram_id,
        caption: caption,
        shortcode: shortcode,
        likes: likes,
        picture_url: picture_url,
        raw: raw
      }
      r.merge!(url: url) if url
      r
    end
  end
end

def load_json_blob
  # Wait what. Where did you get the files from?
  #
  # Opened the instagram page, opened an inspector and Network tab, scrolled all
  # the way down, then right clicked and saved as file each "query"-call. Comme
  # un animal.
  parts = []
  Dir.glob("*").each do |p|
    next if p == "all_instagrams.yml"
    parts << File.open(p).read
  end
  "[#{parts.join(",")}]"
end

# Take a instagram, any instagram URL, and transform it into a a graphdata url
# So you'll get all the metadata.
def fetch_metadata(picture_url)
  body = Net::HTTP.get(URI(picture_url))

  # Instagram sets a window._sharedData object on the page which contains every
  # bit of metadata we need.
  #
  # Get the line containing the data
  shared_data_line = body.lines.grep(/window._sharedData/).first

  # Remove everything before and including `window._sharedData = ` and remove the ending `</script>` tag
  json = shared_data_line[(shared_data_line.index(" = ") + 3)..(shared_data_line.index("</script>") -2)]

  # Parse the JSON to get it back into ruby-land
  Instascraper::Graphdata.with_url(picture_url, JSON.parse(json))
end

all_pictures = JSON.parse(load_json_blob).map do |node|
  begin
    node["data"]["user"]["edge_owner_to_timeline_media"]["edges"].map do |inner_node|
      # inner_node["node"]["shortcode"]
      # inner_node["node"]["edge_media_to_caption"]["edges"].first["node"]["text"]
      Instascraper::Graphdata.new(inner_node["node"]).to_h
    end
  rescue
  end
end.flatten.compact

all_pictures = []
pictures_to_fetch.each.with_index do |url, index|
  print "Fetching #{index}/#{pictures_to_fetch.count}\r"
  metadata = fetch_metadata(url)
  all_pictures << metadata
end

File.open("all_instagrams.yml", 'w') { |f| YAML.dump(all_pictures, f) }

# BIG DATA NUMBER CRUNCHING

def fetch_for(name)
  all = YAML.load_file("all_instagrams.yml")
  regex = regex_for(name)
  pictures = all.select {|h| h[:caption] =~ regex }
  Dir.mkdir(name) unless File.exists?(name)
  pictures.each do |picture|
    caption = picture[:caption]
    path = "#{name}/%s.png" % caption.downcase.scan(/\w+/).join("-")[0..100]
    url = picture[:picture_url]
    `curl -o #{path} #{url}`
  end
end

def fetch_franz
  fetch_for("franz")
end

def fetch_bob
  fetch_for("bob")
end

def regex_for(name)
  if name == "franz"
    /le franz/i
  else
    /dear diary/i
  end
end

def stats_for(name)
  regex = regex_for(name)
  all = YAML.load_file("all_instagrams.yml").select {|h| h[:caption] =~ regex }
  {
    "Total #" => all.count,
    "Average likes" => (all.map {|h| h[:likes]}.inject(:+)).to_f/all.count,
    "Most popular" => all.max_by {|h| h[:likes]}[:shortcode]
  }
end
# results = Instascraper::Graphdata.all_from_yaml(YAML.load_file("all_instagrams.yml"))
# franzies = results.franzies
# bobs = results.bobs
# r = franzis.map{|f| caption = f.caption.match(/as (?:an? )?([^\.]+)/)[0]; [caption, f.likes, f.picture_url] }.sort {|a,b| a[1] <=> b[1]}.reverse
